# -*- coding: utf-8 -*-
"""LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TqR_uHeJBz3KVFEvej6Xprbl5yCaeT2u
"""

!pip install transformers datasets evaluate -q

import os
os.environ["WANDB_DISABLED"] = "true"


from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from datasets import load_dataset
import evaluate
import transformers
from packaging.version import parse

print("transformers version:", transformers.__version__)


MODEL_CHECKPOINT = "distilbert-base-uncased"
MAX_LENGTH = 256
TRAIN_SUBSET = 5000
EVAL_SUBSET = 1000
BATCH_SIZE = 16
NUM_EPOCHS = 2
LEARNING_RATE = 2e-5
OUTPUT_DIR = "./distilbert-imdb-results"


raw_datasets = load_dataset("imdb")
print(raw_datasets)


tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)


def tokenize_function(batch):
    return tokenizer(batch["text"], truncation=True, max_length=MAX_LENGTH)

tokenized = raw_datasets.map(tokenize_function, batched=True, remove_columns=["text"])


if TRAIN_SUBSET is not None:
    train_dataset = tokenized["train"].shuffle(seed=42).select(range(TRAIN_SUBSET))
else:
    train_dataset = tokenized["train"].shuffle(seed=42)

if EVAL_SUBSET is not None:
    eval_dataset = tokenized["test"].select(range(EVAL_SUBSET))
else:
    eval_dataset = tokenized["test"]

print("train size:", len(train_dataset), "eval size:", len(eval_dataset))


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="macro")["f1"],
    }


tf_ver = parse(transformers.__version__)
training_kwargs = dict(
    output_dir=OUTPUT_DIR,
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    weight_decay=0.01,
    logging_dir=f"{OUTPUT_DIR}/logs",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
)


if tf_ver >= parse("4.46"):
    training_kwargs["eval_strategy"] = "epoch"
    training_kwargs["save_strategy"] = "epoch"
else:
    training_kwargs["evaluation_strategy"] = "epoch"
    training_kwargs["save_strategy"] = "epoch"

training_args = TrainingArguments(**training_kwargs)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset.select(range(5000)),
    eval_dataset=eval_dataset.select(range(1000)),
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)


trainer.train()


metrics = trainer.evaluate(eval_dataset=eval_dataset)
print("Final evaluation:", metrics)


trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("Saved model to", OUTPUT_DIR)


examples = [
    "This movie was a fantastic masterpiece, I loved the acting and story.",
    "I wasted two hours. The plot is boring and the acting was terrible.",
]
inputs = tokenizer(examples, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors="pt")
outputs = model(**inputs)
preds = outputs.logits.argmax(axis=-1).tolist()
for ex, p in zip(examples, preds):
    print(p, "->", ex)
